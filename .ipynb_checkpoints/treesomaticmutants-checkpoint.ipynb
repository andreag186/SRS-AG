{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fc6946-3f8f-487b-ac20-ccc9d58e56e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Jupyter Adaptation of Orr et al., (2020) Genomic Workflow**\n",
    "### Author : Andrea Grecu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c04c8d-eea7-45fb-8086-dd44c6f07d67",
   "metadata": {},
   "source": [
    "## **Background | Pūtake**\n",
    "\n",
    "Orr et al., (2020) described a phylogenomic method to measure somatic mutations within a phenotypically mosaic plant individual and further predict the somatic mutation rate within that individual. Both the bioinformatic workflow (*aka. pipeline*) and inputted data used are open-access and found within the orginal journal publication linked below.\n",
    "\n",
    "[Orr et al., ORIGINAL!](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7126060/)\n",
    "\n",
    "The original pipeline created by Orr et al., (2020) was published within a github repository utilizing many bioinformatics tools which cumitavley written in  5+ programming languages. The suggested method to replicate this pipeline is via using the makefiles provided.\n",
    "\n",
    "### Āheitanga\n",
    "\n",
    "***While the original pipeline is open access, it is not particulary user friendly.***\n",
    "\n",
    "Computational methods within bioinformatics often fall short in reproducbility due to insufficient or inacessible documentation (Birmingham, 2017). It is imperative to the progression of bioinformatics research that an effort is made to produce interactive methods which are accessible to a wider audience with limited computer literacy. \n",
    "\n",
    "> ### Thus, the purpose of this notebook is to provide an easy to follow replication of the original pipeline created by Orr et al., (2020) to detect somatic mutations. \n",
    ">The notebook provides a *computational narrative* incorporating inputs, outputs and easy to understand explanations of each step. \n",
    "> By detailing the assumptions,logic, necessary input data and expected ouput this notebook should enable this pipeline to be applied to a new set of collected data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479475f-03ac-4f6a-9695-bf89e1dd7bdf",
   "metadata": {},
   "source": [
    "## **Pipeline Technological Limitations | Tepenga**\n",
    "\n",
    "Considering this pipeline utilizes many (24+) whole genome reads, it requires great computational capacity which restricts the technology which it can be succesfully run on.\n",
    "\n",
    "While some changes can be made to the scripts to adapt for different RAM and thread values of your device, it is not recommended to run this pipeline on a device with capacity much lower than the default (*as this will likely take a very long time to run and much of the memory of your device*). \n",
    "\n",
    "> ### Default Capacity\n",
    "> **RAM = 64 GB**\n",
    ">\n",
    "> **CPU = 48 Threads (20 IS MININMUM RECOMMENDED -> CAN BE ADJUSTED)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc030401-943e-498b-b82d-8cabeceb38d6",
   "metadata": {},
   "source": [
    "## **Required Software Installation |  Tāuta**\n",
    "\n",
    "Prior to running this pipeline, the appropiate software's must be installed. \n",
    "This can be done directly within this notebook by running the cells below.\n",
    "\n",
    "*ASSUMING YOU HAVE THE [CONDA PACKAGE MANAGER](https://docs.conda.io/en/latest/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69e977-8284-4e03-b6e3-de6c1d9b05d0",
   "metadata": {},
   "source": [
    "#### Bioconda \n",
    "Most of the packages used further along in this pipeline are provided via the **Bioconda** channel (package manager). \n",
    "\n",
    "More information about bioconda can be found [here](https://bioconda.github.io/)\n",
    "\n",
    "***To install bioconda run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e4a15d-f2be-4be9-9d5c-0b849b566bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'defaults' already in 'channels' list, moving to the top\n",
      "Warning: 'bioconda' already in 'channels' list, moving to the top\n",
      "Warning: 'conda-forge' already in 'channels' list, moving to the top\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "!conda config --add channels defaults\n",
    "!conda config --add channels bioconda\n",
    "!conda config --add channels conda-forge\n",
    "!conda config --set channel_priority strict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437073e-10b3-439b-97e6-e802cacf338d",
   "metadata": {},
   "source": [
    "#### Khmer\n",
    "\n",
    "The khmer software allows for nucleotide sequence analysis, and is necessary to build a kmer count graph in step 2 for slicing reads (Crusoe et al., 2015). \n",
    "\n",
    "More information on Khmer can be found [here](https://github.com/dib-lab/khmer)\n",
    "\n",
    "***To install Khmer run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5924bebf-68b9-4c3b-b033-50a47224db79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} khmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2ea08-92b5-4652-a4c2-83b65ceaebac",
   "metadata": {},
   "source": [
    "#### Rcorrector\n",
    "\n",
    "The Rcorrector software allows for the correction of Illumina RNA-seq , and is necessary for cleaning reads in step 1(Song & Florea, 2015). \n",
    "\n",
    "More information on Rcorrector can be found [here](https://github.com/mourisl/Rcorrector)\n",
    "\n",
    "***To install Rcorrector run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1258991f-4456-4d7e-b6b5-652336e19b80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} rcorrector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3749a0-6e7a-4819-8b81-cd31563d6072",
   "metadata": {},
   "source": [
    "#### NextGenMap\n",
    "\n",
    "The NextGenMap software allows for short read mapping with a high sensitivity threshold, and is necessary for step ?* (Sedlazeck et al., 2013).\n",
    "\n",
    "More information on NextGenMap can be found [here](https://github.com/Cibiv/NextGenMap/wiki)\n",
    "\n",
    "***To install NextGenMap run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1841983f-b3fa-44e6-a0b0-6aba5dbeaa26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} nextgenmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40336b83-39fb-49df-97f0-89aadbb95d48",
   "metadata": {},
   "source": [
    "#### GNU Parallel\n",
    "\n",
    "The GNU Parallel operating system is a free software used to conduct jobs in parallel, and is necessary for step ?* (Tange, 2018).\n",
    "\n",
    "More information on GNU Parallel can be found [here](https://www.gnu.org/software/parallel/)\n",
    "\n",
    "***To install GNU Parallel run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c7fba73-5faa-4f73-b994-5ebbf070b744",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc27d7-99f6-406c-a504-f01b364fcb76",
   "metadata": {},
   "source": [
    "#### Samtools\n",
    "\n",
    "The Samtools software enables manipulation of next-generation sequencing data (Danecek et al., 2021).\n",
    "\n",
    "More information on Samtools can be found [here](https://github.com/samtools/samtools)\n",
    "\n",
    "***To install Samtools run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d7ea843-b4b0-4ba0-a274-bb7433663693",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} samtools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a9546-90a9-4c6a-9439-11fd0507f5c0",
   "metadata": {},
   "source": [
    "#### BCFtools\n",
    "\n",
    "The BCFtools software provides commands used by Samtools and HTSlib which are adjacently installed (Danecek et al., 2021).\n",
    "\n",
    "More information on BCFtools can be found [here](https://github.com/samtools/bcftools)\n",
    "\n",
    "***To install BFCtools run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a586e825-fbdb-4d1f-9202-36caa131d678",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} bcftools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e52d2e-88d2-4057-b050-17faa48ea7d1",
   "metadata": {},
   "source": [
    "#### HTSlib\n",
    "\n",
    "The HTSlib is a C library used for high-throughput sequencing formats used within Samtools and BCFtools (Bonfield et al., 2021).\n",
    "\n",
    "More information on HTSlib can be found [here](https://github.com/samtools/htslib)\n",
    "\n",
    "***To install HTSlib run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15f14fb8-64dc-433c-92c7-64625d1e2735",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} htslib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f8ff0-51c6-4ab0-be2e-65a0ddd1f151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GATK\n",
    "\n",
    "GATK is a **G**enome **A**nalysis **T**ool**K**it designed to identify variants in genomes and is used throughout the pipeline (O’Connor & Van der Auwera, 2020).\n",
    "\n",
    "More information on GATK can be found [here](https://gatk.broadinstitute.org/hc/en-us)\n",
    "\n",
    "***To install GATK run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d87fc2d0-3f2e-41bb-817f-6e1a049fb874",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} gatk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649da02-74ca-4d3d-b4ed-d55228f52d68",
   "metadata": {},
   "source": [
    "#### RAxML\n",
    "\n",
    "RAxML is a **R**andomized **A**xelerated **M**aximised **L**ikelihood algorithim enabling maximum likelihood phylogenetic tree searches and is used in step ?* (Stamatakis, 2006).\n",
    "\n",
    "More information on RAxML can be found [here](https://cme.h-its.org/exelixis/web/software/raxml/index.html)\n",
    "\n",
    "***To install RAxML run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f06b05b-61e5-49bf-af51-e9e58de17b2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} raxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9610524-f1dd-4cd3-bdf5-18e0b3cb782a",
   "metadata": {},
   "source": [
    "#### Bedtools\n",
    "\n",
    "The bedtools software encompasses many algorithimic programmes used for genome analysis and is used !come back (Quinlan & Hall, 2010).\n",
    "\n",
    "More information on bedtools can be found [here](https://bedtools.readthedocs.io/en/latest/)\n",
    "\n",
    "***To install Bedtools run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c00407b-fd26-4890-9415-e7b5b4bc2fa2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} bedtools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e316aa6-e49a-49c0-844e-1a769f9eecf7",
   "metadata": {},
   "source": [
    "#### UCSC LiftOver\n",
    "\n",
    "LiftOver is a tool provided within the UCSC Genome Browser used to collate genetic analyses to the same build, version or collate assemblies (Hinrichs et al., 2006). LiftOver is used in step of the pipeline\n",
    "\n",
    "More information on LiftOver can be found [here](http://hgdownload.cse.ucsc.edu/admin/exe/)\n",
    "\n",
    "***To install LiftOver run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b0b78d2-6979-4103-9caa-ca96fa93ea66",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} ucsc-liftover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff7090-d444-4e07-9c4d-5abba54e574c",
   "metadata": {},
   "source": [
    "#### VCFtools\n",
    "\n",
    "The VCFtools program package enables operations such as filtering and categorising variants of VCF (**V**ariant **C**all **F**ormat) files and is used in step ?* (Danecek et al., 2011).\n",
    "\n",
    "More information on VCFtools can be found [here](https://vcftools.github.io/)\n",
    "\n",
    "***To install VCFtools run the code cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d412a89f-361d-495f-bf54-e32be35c712b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} vcftools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab8d3e-b707-4a2d-a144-b235c0908d58",
   "metadata": {},
   "source": [
    "#### Picard\n",
    "\n",
    "Picard is a package of command line tools for manipulation of HTS (**H**igh **T**hroughput **S**equencing) data, including identification of duplicated reads as utilized in step 5 (Picard Toolkit, 2018).\n",
    "\n",
    "More information on Picard can be found [here](https://broadinstitute.github.io/picard/)\n",
    "\n",
    "***To install Picard run the code cell below***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bebc29e-5ff8-4328-a9f4-9e9307d427e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/agre945/.conda/envs/miss-molly\n",
      "\n",
      "  added / updated specs:\n",
      "    - picard\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    picard-2.18.29             |                0        13.6 MB  bioconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        13.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  picard             bioconda/noarch::picard-2.18.29-0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "picard-2.18.29       | 13.6 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} picard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e44263-618b-4b86-855c-597aa47345a4",
   "metadata": {},
   "source": [
    "## **Data Input Requirements | Tāuru Raraunga**\n",
    "\n",
    "### Data Collection \n",
    "\n",
    "Orr et al.,(2020) sampled data from a phenotypically mosaic *Eucalyptus melliodora* individual (aka. a Yellow Box tree) in which it was known that one branch expressed resistance to defoliation via the *Anoploganthus* or Christmas beetle genus. From 8 distinct branches, 3 replicate samples were taken from the leaf tip in order to sequence the full genome. \n",
    "\n",
    "Data collected to be used within this pipeline is not restrictive in the number of samples but rather the number of replicates per sample;\n",
    "\n",
    "> #### **!!! TIP !!!**\n",
    ">*There must be at least 2 replicates per sample and an equal number of replicates across all samples. Each sample should be sequenced fully to produce a genome via Illumina.\n",
    "The suffix of the each replicate for one sample should be formatted as follows :*\n",
    "**Sample1a, Sample1b, Sample1c etc.**\n",
    ">\n",
    ">*The raw data inputted should be paired sequencing reads for each sample in FASTQ format. The suffix of the pair files per replicate should be* **\"R1.fastq\"** *and* **\"R2.fastq\"**\n",
    ">\n",
    ">> Raw Data should be deposited in the raw folder (/rep_files/data/raw/), in a new folder \"my_data\". You will see the raw data used by Orr et al.,(2020) in the raw folder which will be used by default.\n",
    "Each **replicate** of one **sample** should have two files in the following format: ***\"Sample1a_R1.fastq\"*** !!!!!!\n",
    "\n",
    "As always, considerations into collecting and using data in a respectful and responsible manner towards communities (across all taxa) should take precedence. \n",
    "\n",
    "### Pseudogenome\n",
    "\n",
    "There was no high-quality reference genome available for *E. melliodora* thus a pseudoreference genome was created using the reference genome of the closely related *Eucalyptus grandis* aka. Rose Gum tree (Bartholomé et al., 2014). \n",
    "> #### **!!! TIP !!!**\n",
    ">*If replicating with your own data, use either the most closely related high quality reference genome available for your sampled taxa OR for your exact taxa if available.*\n",
    ">\n",
    ">*The genome should be in a Fa-file format within a new folder inside the data folder labelled \"my_ref\" (rep_files/data/my_ref/) and the file itself called \"ref.fa\".*\n",
    ">\n",
    ">The *E.grandis* genome found in the \"e_grandis\" folder (/rep_files/data/e_grandis/) will be used by default otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b40d6-548a-40f2-9a78-c138d499dfc8",
   "metadata": {},
   "source": [
    "# **Analysis Makefile**\n",
    "The first makefile that Orr et al., (2020) suggests executing is found within the analysis folder of the repository files. This makefile uses 4 different scripts from the scripts folder, and can be broken down into **? main steps**.\n",
    "The suggested makefile is quite large... Running the entire makefile would take at least a week!\n",
    "\n",
    "Each step also has unique dependencies, paramaters and input/output specifications that are not detailed within the makefile.\n",
    "Provenance tracking throughout each step of a pipeline is vital to it's reproducibility as well as reusability (Kanwal et al., 2017).\n",
    "\n",
    "**Thus, the original analysis makefile has been broken up into several smaller makefiles**\n",
    "These makefiles are found in folders named after which step(s) they execute in the analysis folder e.g. (rep_files/analysis/steps1_2).\n",
    "\n",
    "> *Below are descriptions of each step considering inputs, ouputs, set variables and any potential errors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0528f1-45c5-422d-a178-d203cd8cf1b0",
   "metadata": {},
   "source": [
    "## **Makefile One | Steps 1-3**\n",
    "The first  Makefile to run is within the folder \"steps1_3\"\n",
    "\n",
    "The details of each step are detailed below.\n",
    "\n",
    "To run this makefile run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d15bee77-c918-41db-af9b-69860b3d2608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘./data/raw/’: No such file or directory\n",
      "make: *** No targets.  Stop.\n"
     ]
    }
   ],
   "source": [
    "# Run makefile steps 1 to 2\n",
    "\n",
    "# Get correct directory\n",
    "import os\n",
    "os.chdir('/scale_wlg_nobackup/filesets/nobackup/uoa03626/ag-som-variation/SRS-AG/rep_files/analysis/steps1_3')\n",
    "\n",
    "#Input Path below to rep_files folder\n",
    "\n",
    "# Execute makefile\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e226c-4089-4b5f-bf54-fdb0f25b03e6",
   "metadata": {},
   "source": [
    "### **Step One | Read Correction**\n",
    "The first step of the pipeline is to *correct* the raw reads.\n",
    "\n",
    "This step utilizes the algorthims provided by the **[Rcorrector](https://www.researchgate.net/publication/283260409_Rcorrector_efficient_and_accurate_error_correction_for_Illumina_RNA-seq_reads)** software to determine trusted kmers (using a De Brujin Graph) which will be further used in the next step to correct random sequencing errors producing sliced reads.\n",
    "\n",
    "#### **Input Data**\n",
    "> Raw reads with the suffix \"R1.fastq\" within the directory specified via the **READSFOLDER** variable are utilized. *I.e. the first of the paired reads for one replicate*. By default this is **\"../data/raw/\"**.\n",
    "> \n",
    "> If using your own raw reads, you may want to change the directory utilising the code in the code cell below.\n",
    "\n",
    "#### **Scripts Used**\n",
    "> Lines 100-103 of the script clean_reads.sh execute this step. \n",
    ">\n",
    "> The clean_reads.sh script is called in line 25 of the Makefile. (This utilizies the directories set via **SCRIPTDIR** and **CLEANREADS**).\n",
    ">\n",
    "> By default this script **utilises 48 threads**. However, this can be changed in the code cell below.\n",
    ">\n",
    "> *A reminder that it is not recommended to set the threads below 20 as this will result in a very long runtime... (2 days +)*\n",
    "\n",
    "#### **Output Produced**\n",
    "> The corrected reads will be outputted in a folder named \"corrected\"found in the \"./cleaned_reads\" folder.\n",
    "> \n",
    "> The code in the cell below can be used to check if the correct files have been  produced in the corrected folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7416a38-0b61-488e-a356-f70351e13f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRR9650833_RRR1.cor.fq\tSRR9650841_RRR1.cor.fq\tSRR9650849_RRR1.cor.fq\n",
      "SRR9650833_RRR2.cor.fq\tSRR9650841_RRR2.cor.fq\tSRR9650849_RRR2.cor.fq\n",
      "SRR9650834_RRR1.cor.fq\tSRR9650842_RRR1.cor.fq\tSRR9650850_RRR1.cor.fq\n",
      "SRR9650834_RRR2.cor.fq\tSRR9650842_RRR2.cor.fq\tSRR9650850_RRR2.cor.fq\n",
      "SRR9650835_RRR1.cor.fq\tSRR9650843_RRR1.cor.fq\tSRR9650851_RRR1.cor.fq\n",
      "SRR9650835_RRR2.cor.fq\tSRR9650843_RRR2.cor.fq\tSRR9650851_RRR2.cor.fq\n",
      "SRR9650836_RRR1.cor.fq\tSRR9650844_RRR1.cor.fq\tSRR9650852_RRR1.cor.fq\n",
      "SRR9650836_RRR2.cor.fq\tSRR9650844_RRR2.cor.fq\tSRR9650852_RRR2.cor.fq\n",
      "SRR9650837_RRR1.cor.fq\tSRR9650845_RRR1.cor.fq\tSRR9650853_RRR1.cor.fq\n",
      "SRR9650837_RRR2.cor.fq\tSRR9650845_RRR2.cor.fq\tSRR9650853_RRR2.cor.fq\n",
      "SRR9650838_RRR1.cor.fq\tSRR9650846_RRR1.cor.fq\tSRR9650854_RRR1.cor.fq\n",
      "SRR9650838_RRR2.cor.fq\tSRR9650846_RRR2.cor.fq\tSRR9650854_RRR2.cor.fq\n",
      "SRR9650839_RRR1.cor.fq\tSRR9650847_RRR1.cor.fq\tSRR9650855_RRR1.cor.fq\n",
      "SRR9650839_RRR2.cor.fq\tSRR9650847_RRR2.cor.fq\tSRR9650855_RRR2.cor.fq\n",
      "SRR9650840_RRR1.cor.fq\tSRR9650848_RRR1.cor.fq\tSRR9650856_RRR1.cor.fq\n",
      "SRR9650840_RRR2.cor.fq\tSRR9650848_RRR2.cor.fq\tSRR9650856_RRR2.cor.fq\n"
     ]
    }
   ],
   "source": [
    "# STEP ONE CODE\n",
    "# TIP - Remove hash from code below and run cell to execute!\n",
    "\n",
    "# Use own raw data \n",
    "#!sed -i '/READSFOLDER=/c\\READSFOLDER=*path/to/your/raw/data*' *your/path/here*/analysis/Makefile\n",
    "\n",
    "# Change Default Threads Used\n",
    "#!sed -i '/THREADS=48/c\\THREADS=*VALUE*' *your/path/here*/clean_reads.sh \n",
    "\n",
    "# Check Corrected Reads \n",
    "#import os\n",
    "#os.chdir ('your/path/here/rep_files/analysis/cleaned_reads/corrected')\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32130ea3-fd8b-4758-a2f6-a9c0f6b42174",
   "metadata": {},
   "source": [
    "### **Step Two | Khmer Count Graph**\n",
    "The second step is to store trusted kmers in a khmer graph, which will then be used to filter corrected reads which are excessively high coverage to reduce errors when mapping reads to the reference genome. This is done using the [Khmer](https://www.researchgate.net/publication/282249513_The_khmer_software_package_Enabling_efficient_nucleotide_sequence_analysis) software.\n",
    "\n",
    "#### **Input Data**\n",
    "> Corrected reads produced from step one which were deposited in the \"./cleaned_reads/corrected\" folder are used. \n",
    "\n",
    "#### **Scripts Used**\n",
    "> Line 106 of the script clean_reads.sh executes this step, calling on the script load-into-counting-COR.py provided (due to changes in Khmer software scripts).\n",
    ">\n",
    "> The clean_reads.sh script is called in line 25 of the Makefile\n",
    "> (This utilizies the directories set via **SCRIPTDIR** and **CLEANREADS**).\n",
    "\n",
    "#### **Outputs Produced**\n",
    "> The Khmer graph itself is ouputted within the \"./cleaned_reads\" folder in a Binary file labelled **\"khmer_count.graph\"** alongside a textfile labelled **\"khmer_count.graph.info\"**.\n",
    ">\n",
    "> The textfile displays which khmer software version is installed, which files kmers were obtained from, the total number of unique kmers obtained and the **fp** *(false positive)* **rate**.\n",
    ">\n",
    "> If the last two values are displayed as zero, it is likely that an error occured!\n",
    "> **Check the values by running the code in the cells below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153ed0e8-056c-44e3-b986-51910d9ff4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique k-mers: 4536784228\n",
      "fp rate estimated to be 0.004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP TWO CODE\n",
    "\n",
    "# Run the lines below to check the fp rate and total khmer value\n",
    "\n",
    "import os\n",
    "os.getcwd() #run this line first to get your own cleaned_reads folder file path and replace below\n",
    "os.chdir('[your path]/rep_files/analysis/cleaned_reads')\n",
    "os.getcwd()# run to check working directory correct\n",
    "!tail -3 khmer_count.graph.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732aa26a-1252-48c1-9015-6d8161addda7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Step Three | Read Slicing**\n",
    "The third step of the pipeline is to slice the corrected reads, removing reads which have high or low coverage level. \n",
    "\n",
    "The default maximum coverage specified is 40000, however, this can be altered in the cell below. \n",
    "\n",
    "*Information on filtering reads using coverage and what threshold might be appropiate for your data can be found [here](https://khmer-recipes.readthedocs.io/en/latest/001-extract-reads-by-coverage/).*\n",
    "\n",
    "#### **Input Data**\n",
    "> Corrected reads produced from step one which were deposited in the \"./cleaned_reads/corrected\" folder are used as well as their respective unique kmers stored in the khmer_count.graph file in the same folder.\n",
    "\n",
    "#### **Scripts Used**\n",
    "> Lines 108-122 of the script clean_reads.sh executes this step, calling on the script slice-paired-reads-coverage.py provided (altered from khmer's script to handle paired-end reads). \n",
    ">\n",
    "> The clean_reads.sh script is called in line 25 of the Makefile\n",
    "> (This utilizies the directories set via **SCRIPTDIR** and **CLEANREADS**).\n",
    "\n",
    "#### **Outputs Produced**\n",
    "> Paired reads which passed the coverage filtering will be outputted in a folder \"sliced\" found in the \"./cleaned_reads\" folder. \n",
    ">\n",
    "> Reads which passed filtering while their pair did not will also be outputted in the sliced folder and can be identified with the suffix \".cor_singletons.fastq\"\n",
    ">\n",
    "> The code in the cell below can be used to check if the correct files have been  produced in the sliced folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebaf0715-cd84-40f5-b296-dc82d03bd0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRR9650833_RR.cor_singletons.fastq  SRR9650845_RR.cor_singletons.fastq\n",
      "SRR9650833_RRR1.cor_sliced.fastq    SRR9650845_RRR1.cor_sliced.fastq\n",
      "SRR9650833_RRR2.cor_sliced.fastq    SRR9650845_RRR2.cor_sliced.fastq\n",
      "SRR9650834_RR.cor_singletons.fastq  SRR9650846_RR.cor_singletons.fastq\n",
      "SRR9650834_RRR1.cor_sliced.fastq    SRR9650846_RRR1.cor_sliced.fastq\n",
      "SRR9650834_RRR2.cor_sliced.fastq    SRR9650846_RRR2.cor_sliced.fastq\n",
      "SRR9650835_RR.cor_singletons.fastq  SRR9650847_RR.cor_singletons.fastq\n",
      "SRR9650835_RRR1.cor_sliced.fastq    SRR9650847_RRR1.cor_sliced.fastq\n",
      "SRR9650835_RRR2.cor_sliced.fastq    SRR9650847_RRR2.cor_sliced.fastq\n",
      "SRR9650836_RR.cor_singletons.fastq  SRR9650848_RR.cor_singletons.fastq\n",
      "SRR9650836_RRR1.cor_sliced.fastq    SRR9650848_RRR1.cor_sliced.fastq\n",
      "SRR9650836_RRR2.cor_sliced.fastq    SRR9650848_RRR2.cor_sliced.fastq\n",
      "SRR9650837_RR.cor_singletons.fastq  SRR9650849_RR.cor_singletons.fastq\n",
      "SRR9650837_RRR1.cor_sliced.fastq    SRR9650849_RRR1.cor_sliced.fastq\n",
      "SRR9650837_RRR2.cor_sliced.fastq    SRR9650849_RRR2.cor_sliced.fastq\n",
      "SRR9650838_RR.cor_singletons.fastq  SRR9650850_RR.cor_singletons.fastq\n",
      "SRR9650838_RRR1.cor_sliced.fastq    SRR9650850_RRR1.cor_sliced.fastq\n",
      "SRR9650838_RRR2.cor_sliced.fastq    SRR9650850_RRR2.cor_sliced.fastq\n",
      "SRR9650839_RR.cor_singletons.fastq  SRR9650851_RR.cor_singletons.fastq\n",
      "SRR9650839_RRR1.cor_sliced.fastq    SRR9650851_RRR1.cor_sliced.fastq\n",
      "SRR9650839_RRR2.cor_sliced.fastq    SRR9650851_RRR2.cor_sliced.fastq\n",
      "SRR9650840_RR.cor_singletons.fastq  SRR9650852_RR.cor_singletons.fastq\n",
      "SRR9650840_RRR1.cor_sliced.fastq    SRR9650852_RRR1.cor_sliced.fastq\n",
      "SRR9650840_RRR2.cor_sliced.fastq    SRR9650852_RRR2.cor_sliced.fastq\n",
      "SRR9650841_RR.cor_singletons.fastq  SRR9650853_RR.cor_singletons.fastq\n",
      "SRR9650841_RRR1.cor_sliced.fastq    SRR9650853_RRR1.cor_sliced.fastq\n",
      "SRR9650841_RRR2.cor_sliced.fastq    SRR9650853_RRR2.cor_sliced.fastq\n",
      "SRR9650842_RR.cor_singletons.fastq  SRR9650854_RR.cor_singletons.fastq\n",
      "SRR9650842_RRR1.cor_sliced.fastq    SRR9650854_RRR1.cor_sliced.fastq\n",
      "SRR9650842_RRR2.cor_sliced.fastq    SRR9650854_RRR2.cor_sliced.fastq\n",
      "SRR9650843_RR.cor_singletons.fastq  SRR9650855_RR.cor_singletons.fastq\n",
      "SRR9650843_RRR1.cor_sliced.fastq    SRR9650855_RRR1.cor_sliced.fastq\n",
      "SRR9650843_RRR2.cor_sliced.fastq    SRR9650855_RRR2.cor_sliced.fastq\n",
      "SRR9650844_RR.cor_singletons.fastq  SRR9650856_RR.cor_singletons.fastq\n",
      "SRR9650844_RRR1.cor_sliced.fastq    SRR9650856_RRR1.cor_sliced.fastq\n",
      "SRR9650844_RRR2.cor_sliced.fastq    SRR9650856_RRR2.cor_sliced.fastq\n"
     ]
    }
   ],
   "source": [
    "# STEP THREE CODE\n",
    "\n",
    "# Change the max coverage to desired value below-> ** = replace\n",
    "#!sed -i '/COVERAGE=/c\\COVERAGE=*VALUE*' *your/path/here*/clean_reads.sh \n",
    "\n",
    "# Check Sliced Reads \n",
    "#!ls [your_path]/rep_files/analysis/cleaned_reads/sliced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c1361-00b1-4c59-b5db-77457d9bfb4d",
   "metadata": {},
   "source": [
    "### **Step Four | Allign Reads to Reference Genome**\n",
    "The fourth step of the pipeline is to align/map reads to the reference genome (*Eucalyptus grandis* in the original exp.) using the [NextGenMap](https://cibiv.github.io/NextGenMap/) algorithim.\n",
    "\n",
    "#### **Input Data**\n",
    "> The input data is denoted by -i in the ngm_aligner.sh script and changes accordingly to which repeat is occuring \n",
    ">> **!!! SEE ITERATIONS OF STEPS FOUR AND FIVE !!!**\n",
    "\n",
    "#### **Scripts Used**\n",
    "> The script ngm_alinger.sh is used for this step, found in the scripts folder of the repository.\n",
    ">\n",
    "> This script is first called in line 47 of the Makefile (*due to iterations*).\n",
    ">\n",
    "> By default this script **utilises 48 threads**. However, this can be changed in the code cell below.\n",
    ">\n",
    "> *A reminder that it is not recommended to set the threads below 20 as this will result in a very long runtime... (2 days +)*\n",
    "\n",
    "#### **Outputs Produced**\n",
    "> The output file of step four is a bam file (*aka. a Binary Alignment Map*) of the aligned reads denoted by the variable -o in ngm_aligner.sh which by default is the reference genome file name and iteration number in the following format **\"e_mel_1.bam\"**. \n",
    ">\n",
    "> Whilst running this script a **temporary file (tmp)** will be produced.\n",
    "> This temporary file is quite large, and by default it stored in the tmp directory of your computer's node. \n",
    "> To change the directory in which the tmp file is produced run the code in the cell below. \n",
    ">\n",
    "> To check if the bam file per iteration has been produced and is not empty run the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9d8295-994e-465d-8b30-cbbbca7b5764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131G\te_mel_1.bam\n"
     ]
    }
   ],
   "source": [
    "# STEP FOUR CODE\n",
    "\n",
    "# Change tmp file directory\n",
    "#!sed -i '/TMPOPTION=\"\"/c\\TMPOPTION=\"your/directory\"' *your/path/here*/ngm_aligner.sh \n",
    "\n",
    "# Change Default Threads Used\n",
    "#!sed -i '/CORES=48/c\\CORES=*VALUE*' *your/path/here*/ngm_aligner.sh\n",
    "\n",
    "# Change directory to folder of current iteration\n",
    "#import os\n",
    "#os.chdir('/your/path/to/analysis/e_mel_*iteration num*')\n",
    "\n",
    "# Check presence and size of bam file\n",
    "!du -bsh *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc790a0d-0630-4315-bff9-7ec5636db762",
   "metadata": {},
   "source": [
    "### **Step Five | Create a Consensus Sequence from the BAM**\n",
    "The fifth step of the pipeline is to create a consensus FASTA sequence using the algorithims [bcftools consensus](https://samtools.github.io/bcftools/howtos/consensus-sequence.html), [picard](https://broadinstitute.github.io/picard/) and [tabix](http://www.htslib.org/doc/tabix.html) both provided by samtools. \n",
    "\n",
    "The final iteration of the consensus sequence will be utilized as the pseudogenome reference genome.\n",
    "\n",
    "#### **Input Data**\n",
    "> The inputted data is the BAM file of the previous iteration produced by step 4 above.\n",
    ">> **!!! SEE ITERATIONS OF STEPS FOUR AND FIVE !!!**\n",
    "\n",
    "#### **Scripts Used**\n",
    "> The script create_consensus.sh is used for this step, found in the scripts folder of the repository. This script is fist called in line 35 of the Makefile \n",
    "> (*due to iterations*).\n",
    "> This script utilises the bam file created in step 5 (of the aligned reads) as well as the reference genome to produce a conensus sequence.\n",
    ">\n",
    "> **IMPORTANT** A default filter in this script IGNORES sites where the number of alternative allele(s) found is equal to 1.\n",
    "> More information on bcftools filters can be found  [here](http://www.htslib.org/doc/1.0/bcftools.html#filter).\n",
    "> This filter can be adjusted in the code cell below.\n",
    ">\n",
    "> By default this script **utilises 48 threads**. However, this can be changed in the code cell below.\n",
    ">\n",
    "> *A reminder that it is not recommended to set the threads below 20 as this will result in a very long runtime... (2 days +)*\n",
    "\n",
    "\n",
    "#### **Outputs Produced**\n",
    "> The consensus FASTA sequence is outputted as a file denoted by -o in create_consensus.sh which by default is theis the reference genome file name and iteration number in the following format **\"e_mel_1.fa\"**. \n",
    ">\n",
    "> Additionally, a chain file documenting each rearrangement between the input BAM file and the output consensus file. By default this has the same name as the outputted consensus sequence with the suffix \".chain\".\n",
    ">\n",
    "> As above in step 4, a large temporary file is produced. To change the directory of this tmp file use the code in the cell below.\n",
    ">\n",
    "> While using the bcf tools algorithim a vcf file containing genotype calls is generated but by default NOT SAVED as an output file. This is because this pipeline rellies on the [GATK best practices](https://gatk.broadinstitute.org/hc/en-us/sections/360007226651)\n",
    "workflow to call the genotype variants further along in the pipeline. This vcf file was used to create the consensus sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea6fb1-799e-4b5b-9ae8-2ecaf4eb0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP FIVE CODE\n",
    "\n",
    "# To adjust the Alternative Allele filter\n",
    "## If you want to remove the filter completely DO NOT SET AC TO 0! -> INSTEAD let FILTER=''\n",
    "## The value of AC will reflect the number of alt alleles that will cause a site to be ignored!\n",
    "# !sed -i '/FILTER='AC==1'/c\\FILTER=''' *your-path-here*/create_consensus.sh\n",
    "\n",
    "# Change tmp file directory\n",
    "#!sed -i '/TMPOPTION=\"\"/c\\TMPOPTION=\"your/directory\"' *your/path/here*/create_consensus.sh \n",
    "\n",
    "# Change Default Threads Used\n",
    "#!sed -i '/CORES=48/c\\CORES=*VALUE*' *your/path/here*/create_consensus.sh\n",
    "\n",
    "# To save the bcf tools generated vcf file \n",
    "# !sed -i '/BCFTOOLSFILE=/c\\BCFTOOLSFILE=\"*your_bcf_file.vcf\"' *your-path-here*/create_consensus.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f86bce-fc28-49f2-93d7-3d4b4d6213cc",
   "metadata": {},
   "source": [
    "### **Iterations of Steps Four and Five**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "410ed7f9-840a-458d-a374-20c117c2adaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scale_wlg_nobackup/filesets/nobackup/uoa03626/ag-som-variation/SRS-AG/rep_files/analysis'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change Directory\n",
    "import os\n",
    "os.getcwd() #run this line first \n",
    "os.chdir('/scale_wlg_nobackup/filesets/nobackup/uoa03626/ag-som-variation/SRS-AG/rep_files/analysis')\n",
    "os.getcwd()# run to check working directory correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9594c08-65a0-40c5-a662-8bd320277748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../scripts/create_consensus.sh -o e_mel_1/e_mel_1.fa -r ../data/e_grandis/ref.fa -i e_mel_1/e_mel_1.bam\n",
      "OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:\n",
      "   87184\n",
      "Try using the -Djava.io.tmpdir= option to select an alternate temp location.\n",
      "\n",
      "INFO\t2023-01-19 15:30:23\tMarkDuplicates\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    MarkDuplicates -INPUT e_mel_1/e_mel_1.bam -OUTPUT /scale_wlg_nobackup/filesets/nobackup/uoa03626/dedup_arg.bam -METRICS_FILE /scale_wlg_nobackup/filesets/nobackup/uoa03626/metrics_t3E.txt -MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 1000 -TMP_DIR /scale_wlg_nobackup/filesets/nobackup/uoa03626/create_consensus.sh_tmp_rhMzIq -MAX_RECORDS_IN_RAM 500000000\n",
      "**********\n",
      "\n",
      "\n",
      "15:30:24.446 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scale_wlg_persistent/filesets/home/agre945/.conda/envs/miss-molly/share/picard-2.18.29-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Thu Jan 19 15:30:24 NZDT 2023] MarkDuplicates MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000 INPUT=[e_mel_1/e_mel_1.bam] OUTPUT=/scale_wlg_nobackup/filesets/nobackup/uoa03626/dedup_arg.bam METRICS_FILE=/scale_wlg_nobackup/filesets/nobackup/uoa03626/metrics_t3E.txt TMP_DIR=[/scale_wlg_nobackup/filesets/nobackup/uoa03626/create_consensus.sh_tmp_rhMzIq] MAX_RECORDS_IN_RAM=500000000    MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true DUPLEX_UMI=false ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Thu Jan 19 15:30:24 NZDT 2023] Executing as agre945@wbh001 on Linux 3.10.0-693.2.2.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_332-b09; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.29-SNAPSHOT\n",
      "INFO\t2023-01-19 15:30:24\tMarkDuplicates\tStart of doWork freeMemory: 502413256; totalMemory: 514850816; maxMemory: 954728448\n",
      "INFO\t2023-01-19 15:30:24\tMarkDuplicates\tReading input file and constructing read end information.\n",
      "INFO\t2023-01-19 15:30:24\tMarkDuplicates\tWill retain up to 3459161 data points before spilling to disk.\n",
      "WARNING\t2023-01-19 15:30:24\tAbstractOpticalDuplicateFinderCommandLineProgram\tA field field parsed out of a read name was expected to contain an integer and did not. Read name: SRR9650844.8712123. Cause: String 'SRR9650844.8712123' did not start with a parsable number.\n",
      "^C\n",
      "make: *** [e_mel_1/e_mel_1.fa] Error 1\n"
     ]
    }
   ],
   "source": [
    "# Export the path for the scripts used FIRST\n",
    "!export PATH=$PATH:scale_wlg_nobackup/filesets/nobackup/uoa03626/ag-som-variation/SRS-AG/rep_files/scripts\n",
    "# Run line below SECOND ** = replace\n",
    "#!sed -i '/LOAD_COUNTING=/c\\LOAD_COUNTING=\"*your-path-here*/load-into-counting-COR.py\"' *your-path-here*/clean_reads.sh\n",
    "#!sed -i '/SLICE_BY_COV=/c\\SLICE_BY_COV=\"*your-path-here*/slice-paired-reads-by-coverage.py\"' *your-path-here*/clean_reads.sh\n",
    "!make"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Miss Molly <3",
   "language": "python",
   "name": "miss-molly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
